{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from hdfs import InsecureClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through the base path and find all the .h5 files that are present there then save them to a csv file in order to load them later into spark\n",
    "\n",
    "TODO: this actually needs to be reworked to create a table based on the hdfs file directory instead of local, however it doesn't matter as long as we don't change the files just run the code and pretend it does the right thing for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'files.csv']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basedir = '../MillionSongSubset/data'\n",
    "os.listdir(basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext='.h5'\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(basedir):\n",
    "    files = glob.glob(os.path.join(root,'*'+ext))\n",
    "    all_files.append(files)\n",
    "\n",
    "flat_list = [item for sublist in all_files for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/files.csv'\n",
    "with open(basedir + file, 'w', newline='') as myfile:\n",
    "    for line in flat_list:\n",
    "        myfile.write(line)\n",
    "        myfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of partitions we want for the rdds\n",
    "num_nodes = 3\n",
    "num_rep = 2 * num_nodes\n",
    "\n",
    "conf = (SparkConf()\n",
    "   .setMaster(\"spark://192.168.2.110:7077\")\n",
    "   .setAppName(\"Group14\")\n",
    "   .set(\"spark.executor.cores\", 2)\n",
    "   .set(\"spark.pytspark.python\",\"python3\"))\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to convert filenames to actual file content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good methods to explore the h5 inread: .keys(), .attrs.items() see documentation of h5py\n",
    "import tables\n",
    "\n",
    "\n",
    "def get_title(file, idx=0):\n",
    "    return file['metadata']['songs']['title'][idx].decode(\"utf-8\")\n",
    "\n",
    "def get_artist_name(file, idx=0):\n",
    "    return file['metadata']['songs']['artist_name'][idx].decode(\"utf-8\")\n",
    "\n",
    "# todo: implement functions to get other attributes here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hdf5_getters\n",
    "import tables\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# right now only mapping for the title needs to be changed to extract all the other features\n",
    "def f(x):\n",
    "    # set up the url where the data is stored (yes it's ugly)\n",
    "    base_hdfs = '/LDSA/data'\n",
    "    url = 'http://192.168.2.110:50070/webhdfs/v1'\n",
    "    op = '?op=OPEN'\n",
    "    full_path = url + base_hdfs + x[2:] + op\n",
    "    # read in song file from hdfs api and then extract the wanted attributes\n",
    "    with h5py.File(io.BytesIO(requests.get(url = full_path).content), 'r') as f:\n",
    "        song = []\n",
    "        song.append(get_title(f))\n",
    "        song.append(get_artist_name(f))\n",
    "    return song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: set the number of partitions here corectly as calculated above in order to maintain speed\n",
    "file_paths = sc.textFile(basedir + file).map(lambda x: f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the rdd (api version 1) to a dataframe (api version 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: add the needed attributes here that we need..\n",
    "df = sqlContext.createDataFrame(file_paths, schema=['title', 'artist_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with the data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
