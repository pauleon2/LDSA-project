{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from hdfs import InsecureClient\n",
    "import numpy as np\n",
    "import tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through the base path and find all the .h5 files that are present there then save them to a csv file in order to load them later into spark\n",
    "\n",
    "TODO: this actually needs to be reworked to create a table based on the hdfs file directory instead of local, however it doesn't matter as long as we don't change the files just run the code and pretend it does the right thing for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'files.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basedir = '../MillionSongSubset/data'\n",
    "os.listdir(basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext='.h5'\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(basedir):\n",
    "    #print(root, dirs, f)\n",
    "    files = glob.glob(os.path.join(root,'*'+ext))\n",
    "    all_files.append(files)\n",
    "\n",
    "flat_list = [item for sublist in all_files for item in sublist]\n",
    "file = '/files.csv'\n",
    "with open(basedir + file, 'w', newline='') as myfile:\n",
    "    for line in flat_list:\n",
    "        myfile.write(line)\n",
    "        myfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of partitions we want for the rdds\n",
    "num_nodes = 3\n",
    "num_rep = 2 * num_nodes\n",
    "#sc.stop()\n",
    "conf = (SparkConf()\n",
    "   .setMaster(\"spark://192.168.2.110:7077\")\n",
    "   .setAppName(\"Group14\")\n",
    "   .set(\"spark.executor.cores\", 2) # if anything >2, does not run\n",
    "   .set(\"spark.pytspark.python\",\"python3\")\n",
    "   .set(\"spark.dynamicAllocation.enabled\", False)  \n",
    "   .set(\"spark.shuffle.service.enabled\", False))\n",
    "\n",
    "#sc = SparkContext(conf = conf)\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to convert filenames to actual file content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good methods to explore the h5 inread: .keys(), .attrs.items() see documentation of h5py\n",
    "\n",
    "def get_title(file, idx=0):\n",
    "    return file['metadata']['songs']['title'][idx].decode(\"utf-8\")\n",
    "\n",
    "def get_artist_name(file, idx=0):\n",
    "    return file['metadata']['songs']['artist_name'][idx].decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_artist_familiarity(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist familiarity from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['artist_familiarity'][songidx]).encode('utf-8', 'ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_artist_hotttnesss(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist hotttnesss from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['artist_hotttnesss'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_artist_id(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return h5['metadata']['songs']['artist_id'][songidx].decode(\"utf-8\")\n",
    "\n",
    "def get_artist_mbid(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist musibrainz id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return h5['metadata']['songs']['artist_mbid'][songidx].decode(\"utf-8\")\n",
    "\n",
    "def get_artist_playmeid(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist playme id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['artist_playmeid'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_artist_7digitalid(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist 7digital id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['artist_7digitalid'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_artist_latitude(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist latitude from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['artist_latitude'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_artist_longitude(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist longitude from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['artist_longitude'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_artist_location(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist location from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return h5['metadata']['songs']['artist_location'][songidx].decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_release(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get release from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return h5['metadata']['songs']['release'][songidx].decode(\"utf-8\")\n",
    "\n",
    "def get_release_7digitalid(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get release 7digital id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['release_7digitalid'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_song_id(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get song id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return h5['metadata']['songs']['song_id'][songidx].decode(\"utf-8\")\n",
    "\n",
    "def get_song_hotttnesss(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get song hotttnesss from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['song_hotttnesss'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_track_7digitalid(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get track 7digital id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['track_7digitalid'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_analysis_sample_rate(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get analysis sample rate from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['analysis_sample_rate'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_audio_md5(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get audio MD5 from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return h5['analysis']['songs']['audio_md5'][songidx].decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_danceability(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get danceability from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['danceability'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_duration(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get duration from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['duration'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_end_of_fade_in(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get end of fade in from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['end_of_fade_in'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_energy(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get energy from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['energy'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_key(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get key from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['key'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_key_confidence(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get key confidence from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['key_confidence'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_loudness(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get loudness from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['loudness'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_mode(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get mode from a HDF5 song file, by default the first song ifn it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['mode'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_mode_confidence(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get mode confidence from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['mode_confidence'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_start_of_fade_out(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get start of fade out from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['start_of_fade_out'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_tempo(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get tempo from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['tempo'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_time_signature(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get signature from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['time_signature'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_time_signature_confidence(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get signature confidence from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['time_signature_confidence'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_track_id(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get track id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['track_id'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_year(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get release year from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['musicbrainz']['songs']['year'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o20.binaryFiles.\n: java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:332)\n\tat org.apache.spark.util.io.ChunkedByteBuffer$$anonfun$writeFully$1.apply(ChunkedByteBuffer.scala:81)\n\tat org.apache.spark.util.io.ChunkedByteBuffer$$anonfun$writeFully$1.apply(ChunkedByteBuffer.scala:69)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.writeFully(ChunkedByteBuffer.scala:69)\n\tat org.apache.spark.storage.DiskStore$$anonfun$putBytes$1.apply(DiskStore.scala:96)\n\tat org.apache.spark.storage.DiskStore$$anonfun$putBytes$1.apply(DiskStore.scala:95)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:69)\n\tat org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:95)\n\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1516)\n\tat org.apache.spark.storage.memory.MemoryStore.org$apache$spark$storage$memory$MemoryStore$$dropBlock$1(MemoryStore.scala:473)\n\tat org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$1.apply$mcVI$sp(MemoryStore.scala:499)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:490)\n\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:92)\n\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:73)\n\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:179)\n\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:186)\n\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:552)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:211)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:914)\n\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1481)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:123)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1489)\n\tat org.apache.spark.rdd.NewHadoopRDD.<init>(NewHadoopRDD.scala:79)\n\tat org.apache.spark.rdd.BinaryFileRDD.<init>(BinaryFileRDD.scala:36)\n\tat org.apache.spark.SparkContext$$anonfun$binaryFiles$1.apply(SparkContext.scala:929)\n\tat org.apache.spark.SparkContext$$anonfun$binaryFiles$1.apply(SparkContext.scala:922)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:699)\n\tat org.apache.spark.SparkContext.binaryFiles(SparkContext.scala:922)\n\tat org.apache.spark.api.java.JavaSparkContext.binaryFiles(JavaSparkContext.scala:258)\n\tat sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-446a8e3c1cba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinaryFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://192.168.2.110:9000\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-446a8e3c1cba>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinaryFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://192.168.2.110:9000\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mbinaryFiles\u001b[0;34m(self, path, minPartitions)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \"\"\"\n\u001b[1;32m    660\u001b[0m         \u001b[0mminPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminPartitions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultMinPartitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         return RDD(self._jsc.binaryFiles(path, minPartitions), self,\n\u001b[0m\u001b[1;32m    662\u001b[0m                    PairDeserializer(UTF8Deserializer(), NoOpSerializer()))\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o20.binaryFiles.\n: java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:332)\n\tat org.apache.spark.util.io.ChunkedByteBuffer$$anonfun$writeFully$1.apply(ChunkedByteBuffer.scala:81)\n\tat org.apache.spark.util.io.ChunkedByteBuffer$$anonfun$writeFully$1.apply(ChunkedByteBuffer.scala:69)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.writeFully(ChunkedByteBuffer.scala:69)\n\tat org.apache.spark.storage.DiskStore$$anonfun$putBytes$1.apply(DiskStore.scala:96)\n\tat org.apache.spark.storage.DiskStore$$anonfun$putBytes$1.apply(DiskStore.scala:95)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:69)\n\tat org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:95)\n\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1516)\n\tat org.apache.spark.storage.memory.MemoryStore.org$apache$spark$storage$memory$MemoryStore$$dropBlock$1(MemoryStore.scala:473)\n\tat org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$1.apply$mcVI$sp(MemoryStore.scala:499)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:490)\n\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:92)\n\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:73)\n\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:179)\n\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:186)\n\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:552)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:211)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:914)\n\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1481)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:123)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1489)\n\tat org.apache.spark.rdd.NewHadoopRDD.<init>(NewHadoopRDD.scala:79)\n\tat org.apache.spark.rdd.BinaryFileRDD.<init>(BinaryFileRDD.scala:36)\n\tat org.apache.spark.SparkContext$$anonfun$binaryFiles$1.apply(SparkContext.scala:929)\n\tat org.apache.spark.SparkContext$$anonfun$binaryFiles$1.apply(SparkContext.scala:922)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:699)\n\tat org.apache.spark.SparkContext.binaryFiles(SparkContext.scala:922)\n\tat org.apache.spark.api.java.JavaSparkContext.binaryFiles(JavaSparkContext.scala:258)\n\tat sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# df = sc.binaryFiles(\"hdfs://192.168.2.110:9000/LDSA/subset\", minPartitions=12)\n",
    "with open(\"/home/ubuntu/files.csv\", \"r\", newline=\"\\n\") as f:\n",
    "    results = [line.split(',')[-1][1:-2] for line in f]\n",
    "results = results[0:100]\n",
    "\n",
    "df = sc.union([sc.binaryFiles(\"hdfs://192.168.2.110:9000\" + f) for f in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Casual',\n",
       "  \"I Didn't Mean To\",\n",
       "  '0.5817937658450281',\n",
       "  '0.4019975433642836',\n",
       "  'ARD7TVE1187B99BFB1',\n",
       "  'e77e51a5-4761-45b3-9847-2051f811e366',\n",
       "  '4479',\n",
       "  '165270',\n",
       "  'nan',\n",
       "  'nan',\n",
       "  'California - LA',\n",
       "  'Fear Itself',\n",
       "  '300848',\n",
       "  'SOMZWCG12A8C13C480',\n",
       "  '0.6021199899057548',\n",
       "  '3401791',\n",
       "  '22050',\n",
       "  'a222795e07cd65b7a530f1346f520649',\n",
       "  '0.0',\n",
       "  '218.93179',\n",
       "  '0.247',\n",
       "  '0.0',\n",
       "  '1',\n",
       "  '0.736',\n",
       "  '-11.197',\n",
       "  '0',\n",
       "  '0.636',\n",
       "  '218.932',\n",
       "  '92.198',\n",
       "  '4',\n",
       "  '0.778',\n",
       "  \"b'TRAAAAW128F429D538'\",\n",
       "  '0'],\n",
       " ['The Box Tops',\n",
       "  'Soul Deep',\n",
       "  '0.6306300375898077',\n",
       "  '0.4174996449709784',\n",
       "  'ARMJAGH1187FB546F3',\n",
       "  '1c78ab62-db33-4433-8d0b-7c8dcf1849c2',\n",
       "  '22066',\n",
       "  '1998',\n",
       "  '35.14968',\n",
       "  '-90.04892',\n",
       "  'Memphis, TN',\n",
       "  'Dimensions',\n",
       "  '300822',\n",
       "  'SOCIWDW12A8C13D406',\n",
       "  'nan',\n",
       "  '3400270',\n",
       "  '22050',\n",
       "  'bb9771eeef3d5b204a3c55e690f52a91',\n",
       "  '0.0',\n",
       "  '148.03546',\n",
       "  '0.148',\n",
       "  '0.0',\n",
       "  '6',\n",
       "  '0.169',\n",
       "  '-9.843',\n",
       "  '0',\n",
       "  '0.43',\n",
       "  '137.915',\n",
       "  '121.274',\n",
       "  '4',\n",
       "  '0.384',\n",
       "  \"b'TRAAABD128F429CF47'\",\n",
       "  '1969'],\n",
       " ['Sonora Santanera',\n",
       "  'Amor De Cabaret',\n",
       "  '0.4873567909281477',\n",
       "  '0.34342837829688244',\n",
       "  'ARKRRTF1187B9984DA',\n",
       "  '7a273984-edd9-4451-9c4d-39b38f05ebcd',\n",
       "  '13951',\n",
       "  '290021',\n",
       "  'nan',\n",
       "  'nan',\n",
       "  '',\n",
       "  'Las Numero 1 De La Sonora Santanera',\n",
       "  '514953',\n",
       "  'SOXVLOJ12AB0189215',\n",
       "  'nan',\n",
       "  '5703798',\n",
       "  '22050',\n",
       "  'fa329738005ca53715d9f7381a0d1fe3',\n",
       "  '0.0',\n",
       "  '177.47546',\n",
       "  '0.282',\n",
       "  '0.0',\n",
       "  '8',\n",
       "  '0.643',\n",
       "  '-9.689',\n",
       "  '1',\n",
       "  '0.565',\n",
       "  '172.304',\n",
       "  '100.07',\n",
       "  '1',\n",
       "  '0.0',\n",
       "  \"b'TRAAADZ128F9348C2E'\",\n",
       "  '0'],\n",
       " ['Adam Ant',\n",
       "  'Something Girls',\n",
       "  '0.6303823341467806',\n",
       "  '0.4542311565706205',\n",
       "  'AR7G5I41187FB4CE6C',\n",
       "  'e188a520-9cb7-4f73-a3d7-2f70c6538e92',\n",
       "  '12697',\n",
       "  '19072',\n",
       "  'nan',\n",
       "  'nan',\n",
       "  'London, England',\n",
       "  'Friend Or Foe',\n",
       "  '287650',\n",
       "  'SONHOTT12A8C13493C',\n",
       "  'nan',\n",
       "  '3226795',\n",
       "  '22050',\n",
       "  '43cd1abd45d5a2dda16a3c65b4963bd4',\n",
       "  '0.0',\n",
       "  '233.40363',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0',\n",
       "  '0.751',\n",
       "  '-9.013',\n",
       "  '1',\n",
       "  '0.749',\n",
       "  '217.124',\n",
       "  '119.293',\n",
       "  '4',\n",
       "  '0.0',\n",
       "  \"b'TRAAAEF128F4273421'\",\n",
       "  '1982'],\n",
       " ['Gob',\n",
       "  'Face the Ashes',\n",
       "  '0.6510456608317947',\n",
       "  '0.40172368550367865',\n",
       "  'ARXR32B1187FB57099',\n",
       "  'c6903a2e-063c-4f91-a284-17b8f421be7b',\n",
       "  '8355',\n",
       "  '30973',\n",
       "  'nan',\n",
       "  'nan',\n",
       "  '',\n",
       "  'Muertos Vivos',\n",
       "  '611336',\n",
       "  'SOFSOCN12A8C143F5D',\n",
       "  '0.6045007385888197',\n",
       "  '6795666',\n",
       "  '22050',\n",
       "  '580a8fe08ef0f1c7734b84547d7a8bc7',\n",
       "  '0.0',\n",
       "  '209.60608',\n",
       "  '0.066',\n",
       "  '0.0',\n",
       "  '2',\n",
       "  '0.092',\n",
       "  '-4.501',\n",
       "  '1',\n",
       "  '0.371',\n",
       "  '198.699',\n",
       "  '129.738',\n",
       "  '4',\n",
       "  '0.562',\n",
       "  \"b'TRAAAFD128F92F423A'\",\n",
       "  '2007'],\n",
       " ['Jeff And Sheri Easter',\n",
       "  'The Moon And I (Ordinary Day Album Version)',\n",
       "  '0.5352927355118197',\n",
       "  '0.385470550555821',\n",
       "  'ARKFYS91187B98E58F',\n",
       "  '79c403f9-5467-4f23-8426-9ca3fc60a115',\n",
       "  '63801',\n",
       "  '432935',\n",
       "  'nan',\n",
       "  'nan',\n",
       "  '',\n",
       "  'Ordinary Day',\n",
       "  '41838',\n",
       "  'SOYMRWW12A6D4FAB14',\n",
       "  'nan',\n",
       "  '444964',\n",
       "  '22050',\n",
       "  '8ee90e90bb8714300574486f379effb5',\n",
       "  '0.0',\n",
       "  '267.7024',\n",
       "  '2.264',\n",
       "  '0.0',\n",
       "  '5',\n",
       "  '0.635',\n",
       "  '-9.323',\n",
       "  '1',\n",
       "  '0.557',\n",
       "  '254.27',\n",
       "  '147.782',\n",
       "  '3',\n",
       "  '0.454',\n",
       "  \"b'TRAAAMO128F1481E7F'\",\n",
       "  '0'],\n",
       " ['Rated R',\n",
       "  'Keepin It Real (Skit)',\n",
       "  '0.5564956019129572',\n",
       "  '0.2619411773808474',\n",
       "  'ARD0S291187B9B7BF5',\n",
       "  '56503d6d-094e-4c28-ae3d-04cc748ade5b',\n",
       "  '-1',\n",
       "  '17970',\n",
       "  'nan',\n",
       "  'nan',\n",
       "  'Ohio',\n",
       "  'Da Ghetto Psychic',\n",
       "  '25824',\n",
       "  'SOMJBYD12A6D4F8557',\n",
       "  'nan',\n",
       "  '276593',\n",
       "  '22050',\n",
       "  '0e574964bf7a546a39e039f6e35aa48a',\n",
       "  '0.0',\n",
       "  '114.78159',\n",
       "  '0.096',\n",
       "  '0.0',\n",
       "  '1',\n",
       "  '0.0',\n",
       "  '-17.302',\n",
       "  '1',\n",
       "  '0.0',\n",
       "  '114.782',\n",
       "  '111.787',\n",
       "  '1',\n",
       "  '0.0',\n",
       "  \"b'TRAAAMQ128F1460CD3'\",\n",
       "  '0'],\n",
       " ['Tweeterfriendly Music',\n",
       "  'Drop of Rain',\n",
       "  '0.8011364469966873',\n",
       "  '0.6055071356905752',\n",
       "  'AR10USD1187B99F3F1',\n",
       "  'd89de379-665d-425c-b2e9-41b95d1edb36',\n",
       "  '-1',\n",
       "  '21128',\n",
       "  'nan',\n",
       "  'nan',\n",
       "  'Burlington, Ontario, Canada',\n",
       "  'Gin & Phonic',\n",
       "  '8876',\n",
       "  'SOHKNRJ12A6701D1F8',\n",
       "  'nan',\n",
       "  '90004',\n",
       "  '22050',\n",
       "  '1741d2c1bd271f10e783634f286746e2',\n",
       "  '0.0',\n",
       "  '189.57016',\n",
       "  '0.319',\n",
       "  '0.0',\n",
       "  '4',\n",
       "  '0.0',\n",
       "  '-11.642',\n",
       "  '0',\n",
       "  '0.16',\n",
       "  '181.023',\n",
       "  '101.43',\n",
       "  '3',\n",
       "  '0.408',\n",
       "  \"b'TRAAAPK128E0786D96'\",\n",
       "  '0'],\n",
       " ['Planet P Project',\n",
       "  'Pink World',\n",
       "  '0.42666785706940524',\n",
       "  '0.33227574659906806',\n",
       "  'AR8ZCNI1187B9A069B',\n",
       "  '19d232b9-b4d7-4dc8-b259-bf65efb655b1',\n",
       "  '-1',\n",
       "  '276891',\n",
       "  'nan',\n",
       "  'nan',\n",
       "  '',\n",
       "  'Pink World',\n",
       "  '358182',\n",
       "  'SOIAZJW12AB01853F1',\n",
       "  '0.26586104921065007',\n",
       "  '3996579',\n",
       "  '22050',\n",
       "  'eda960ea61211d980653d589e8e74c1d',\n",
       "  '0.0',\n",
       "  '269.81832',\n",
       "  '5.3',\n",
       "  '0.0',\n",
       "  '4',\n",
       "  '0.717',\n",
       "  '-13.496',\n",
       "  '1',\n",
       "  '0.652',\n",
       "  '258.99',\n",
       "  '86.643',\n",
       "  '4',\n",
       "  '0.487',\n",
       "  \"b'TRAAARJ128F9320760'\",\n",
       "  '1984'],\n",
       " ['Clp',\n",
       "  'Insatiable (Instrumental Version)',\n",
       "  '0.5505136978482137',\n",
       "  '0.42270564102939107',\n",
       "  'ARNTLGG11E2835DDB9',\n",
       "  '4d96f7d0-2f0e-4e92-ba70-a405f96f8cec',\n",
       "  '-1',\n",
       "  '242273',\n",
       "  'nan',\n",
       "  'nan',\n",
       "  '',\n",
       "  'Superinstrumental',\n",
       "  '692313',\n",
       "  'SOUDSGM12AC9618304',\n",
       "  'nan',\n",
       "  '7684249',\n",
       "  '22050',\n",
       "  'c76afe800148e674a30629a4686d7f9f',\n",
       "  '0.0',\n",
       "  '266.39628',\n",
       "  '0.084',\n",
       "  '0.0',\n",
       "  '7',\n",
       "  '0.053',\n",
       "  '-6.697',\n",
       "  '0',\n",
       "  '0.473',\n",
       "  '261.747',\n",
       "  '114.041',\n",
       "  '4',\n",
       "  '0.878',\n",
       "  \"b'TRAAAVG12903CFA543'\",\n",
       "  '0']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.map(lambda x: read(x[1])).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(x):\n",
    "    with h5py.File(io.BytesIO(x), 'r') as f:\n",
    "        song = []\n",
    "        \n",
    "        song.append(get_artist_name(f))\n",
    "        song.append(get_title(f))\n",
    "        song.append(get_artist_familiarity(f))\n",
    "        song.append(get_artist_hotttnesss(f))\n",
    "        song.append(get_artist_id(f))\n",
    "        song.append(get_artist_mbid(f))\n",
    "        song.append(get_artist_playmeid(f))\n",
    "        song.append(get_artist_7digitalid(f))\n",
    "        song.append(get_artist_latitude(f))\n",
    "        song.append(get_artist_longitude(f))\n",
    "        song.append(get_artist_location(f))\n",
    "        song.append(get_release(f))\n",
    "        song.append(get_release_7digitalid(f))\n",
    "        song.append(get_song_id(f))\n",
    "        song.append(get_song_hotttnesss(f))\n",
    "        song.append(get_track_7digitalid(f))\n",
    "        song.append(get_analysis_sample_rate(f))\n",
    "        song.append(get_audio_md5(f))\n",
    "        song.append(get_danceability(f))\n",
    "        song.append(get_duration(f))\n",
    "        song.append(get_end_of_fade_in(f))\n",
    "        song.append(get_energy(f))\n",
    "        song.append(get_key(f))\n",
    "        song.append(get_key_confidence(f))\n",
    "        song.append(get_loudness(f))\n",
    "        song.append(get_mode(f))\n",
    "        song.append(get_mode_confidence(f))\n",
    "        song.append(get_start_of_fade_out(f))\n",
    "        song.append(get_tempo(f))\n",
    "        song.append(get_time_signature(f))\n",
    "        song.append(get_time_signature_confidence(f))\n",
    "        song.append(get_track_id(f))\n",
    "        song.append(get_year(f))\n",
    "        \n",
    "    return song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group,284446,2020-06-02,09:57,\"/LDSA/subset/A/A/A/TRAAAAW128F429D538.h5\"\n",
      "org.apache.spark.api.java.JavaPairRDD@6d2c54b3\n"
     ]
    }
   ],
   "source": [
    "# todo: set the number of partitions here corectly as calculated above in order to maintain speed\n",
    "\n",
    "fp = \"hdfs://192.168.2.110:9000/LDSA/subset/files.csv\"\n",
    "#print(fp)\n",
    "\n",
    "\n",
    "def binary(x):\n",
    "    return sc.binaryFiles(\"hdfs://192.168.2.110:9000/LDSA/subset\" + x[25:])\n",
    "\n",
    "def h5file(x):\n",
    "    return h5py.File(io.BytesIO(x[0][1]), 'r')\n",
    "\n",
    "def trial(x):\n",
    "    with h5py.File(\"hdfs://192.168.2.110:9000/LDSA/subset\" +x[25:]) as f:\n",
    "        result = f[x[32:]]\n",
    "        return list(result[:])\n",
    "    \n",
    "\n",
    "file_paths = sc.textFile(fp, minPartitions=12)\n",
    "print(file_paths.take(1)[0][25:])\n",
    "#flatMap(lambda x: binary(x))\n",
    "#rdd = sc.union([sc.binaryFiles(f) for f in file_paths])\n",
    "\n",
    "#rdd = file_paths.flatMap(binary)\n",
    "#rdd.take(1)\n",
    "#files.take(1)\n",
    "df = sc.binaryFiles(\"hdfs://192.168.2.110:9000/LDSA/subset\", minPartitions=12) # returns a JavaPairRDD\n",
    "#ff = sqlContext.read.binaryFile(\"hdfs://192.168.2.110:9000/LDSA/subset\")   \n",
    "#print(df.take(1))\n",
    "#df2=df.flatMap(h5file)\n",
    "#df2 = df.map(lambda x: h5file(x))\n",
    "#print(df2.take(1))\n",
    "print(df)\n",
    "\n",
    "\n",
    "files = df.map(lambda x: foo(x[1]))  \n",
    "\n",
    "def foo(data):  \n",
    "    with io.BytesIO(data) as b, h5py.File(b, 'r') as f:  \n",
    "        return list(f) # [\"analysis\", \"metadata\", \"musicbrainz\"]  \n",
    "\n",
    "#print(files.collect())\n",
    "\n",
    "\n",
    "## Old version\n",
    "#file_paths = sc.textFile(fp, minPartitions=12).map(lambda x: f2(x))\n",
    "#print(file_paths)\n",
    "\n",
    "\n",
    "## One file test\n",
    "#file_paths = sc.binaryFiles(\"hdfs://192.168.2.110:9000/LDSA/subset/A/E/E/TRAEELO128F425BD8F.h5\")\n",
    "#a = h5py.File(io.BytesIO(file_paths.take(1)[0][1]), 'r')\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../MillionSongSubset/data/A/E/E/TRAEELO128F425BD8F.h5']\n"
     ]
    }
   ],
   "source": [
    "print(file_paths.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o20.binaryFiles.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$3.apply(TorrentBroadcast.scala:286)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$3.apply(TorrentBroadcast.scala:286)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1848)\n\tat java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)\n\tat org.apache.hadoop.io.WritableUtils.writeCompressedByteArray(WritableUtils.java:75)\n\tat org.apache.hadoop.io.WritableUtils.writeCompressedString(WritableUtils.java:94)\n\tat org.apache.hadoop.io.WritableUtils.writeCompressedStringArray(WritableUtils.java:155)\n\tat org.apache.hadoop.conf.Configuration.write(Configuration.java:2891)\n\tat org.apache.spark.util.SerializableConfiguration$$anonfun$writeObject$1.apply$mcV$sp(SerializableConfiguration.scala:27)\n\tat org.apache.spark.util.SerializableConfiguration$$anonfun$writeObject$1.apply(SerializableConfiguration.scala:25)\n\tat org.apache.spark.util.SerializableConfiguration$$anonfun$writeObject$1.apply(SerializableConfiguration.scala:25)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)\n\tat org.apache.spark.util.SerializableConfiguration.writeObject(SerializableConfiguration.scala:25)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:291)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:291)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-702835de0e95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/files.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinaryFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://192.168.2.110:9000/LDSA/subset\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-702835de0e95>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/files.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinaryFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://192.168.2.110:9000/LDSA/subset\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mbinaryFiles\u001b[0;34m(self, path, minPartitions)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \"\"\"\n\u001b[1;32m    660\u001b[0m         \u001b[0mminPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminPartitions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultMinPartitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         return RDD(self._jsc.binaryFiles(path, minPartitions), self,\n\u001b[0m\u001b[1;32m    662\u001b[0m                    PairDeserializer(UTF8Deserializer(), NoOpSerializer()))\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o20.binaryFiles.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$3.apply(TorrentBroadcast.scala:286)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$3.apply(TorrentBroadcast.scala:286)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1848)\n\tat java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)\n\tat org.apache.hadoop.io.WritableUtils.writeCompressedByteArray(WritableUtils.java:75)\n\tat org.apache.hadoop.io.WritableUtils.writeCompressedString(WritableUtils.java:94)\n\tat org.apache.hadoop.io.WritableUtils.writeCompressedStringArray(WritableUtils.java:155)\n\tat org.apache.hadoop.conf.Configuration.write(Configuration.java:2891)\n\tat org.apache.spark.util.SerializableConfiguration$$anonfun$writeObject$1.apply$mcV$sp(SerializableConfiguration.scala:27)\n\tat org.apache.spark.util.SerializableConfiguration$$anonfun$writeObject$1.apply(SerializableConfiguration.scala:25)\n\tat org.apache.spark.util.SerializableConfiguration$$anonfun$writeObject$1.apply(SerializableConfiguration.scala:25)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)\n\tat org.apache.spark.util.SerializableConfiguration.writeObject(SerializableConfiguration.scala:25)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1154)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:291)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$2.apply(TorrentBroadcast.scala:291)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n"
     ]
    }
   ],
   "source": [
    "files = open(basedir + \"/files.csv\", \"r\")\n",
    "rdd = sc.union([sc.binaryFiles(\"hdfs://192.168.2.110:9000/LDSA/subset\" + f.rstrip(\"n\")[25:]) for f in files])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file =open('../MillionSongSubset/data/files.csv', \"r\", newline=\"\")\n",
    "\n",
    "hdfs_file_paths = [\"hdfs://192.168.2.110:9000/LDSA/subset\" + path[25:].rstrip('\\n') for path in csv_file]\n",
    "\n",
    "print(hdfs_file_paths[:10])\n",
    "files = sc.union([sc.binaryFiles(p) for p in hdfs_file_paths]) #.map(lambda x: foo(x[1]))\n",
    "files.take(1)\n",
    "def foo(data):\n",
    "    with io.BytesIO(data) as b, h5py.File(b, 'r') as f:  \n",
    "        return list(f) # [\"analysis\", \"metadata\", \"musicbrainz\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Convert rdd to dataframe\n",
    "convert the rdd (api version 1) to a dataframe (api version 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['artist_name', 'title', 'artist_familiarity', \n",
    "             'artist_hotttnesss', 'artist_id', 'artist_mbid', \n",
    "             'artist_playmeid', 'artist_7digitalid', 'artist_latitude', \n",
    "             'artist_longitude', 'artist_location', 'release', \n",
    "             'release_7digitalid', \n",
    "             'song_id', 'song_hotnesss', 'track_7digitalid', \n",
    "             'analysis_sample_rate', \n",
    "             'audio_md5', 'danceability', 'duration', 'end_of_fade_in', \n",
    "             'energy', 'key', \n",
    "             'key_confidence', 'loudness', 'mode', 'mode_confidence', \n",
    "             'start_of_fade_out', \n",
    "             'tempo', 'time_signature', \n",
    "             'time_signature_confidence', 'track_id', 'year']\n",
    "df = file_paths.toDF(columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change types\n",
    "from pyspark.sql import types \n",
    "['BinaryType', 'BooleanType', 'ByteType', 'DateType', \n",
    "          'DecimalType', 'DoubleType', 'FloatType', 'IntegerType', \n",
    "           'LongType', 'ShortType', 'StringType', 'TimestampType']\n",
    "changedTypedf = df.withColumn(\"year\", df[\"year\"].cast(\"Integer\"))\\\n",
    "                .withColumn(\"track_id\", df[\"track_id\"].cast(\"Integer\"))\\\n",
    "                .withColumn(\"artist_id\", df[\"artist_id\"].cast(\"Integer\"))\\\n",
    "                .withColumn(\"song_id\", df[\"song_id\"].cast(\"Integer\"))\\\n",
    "                .withColumn(\"duration\", df[\"duration\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"danceability\", df[\"danceability\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"end_of_fade_in\", df[\"end_of_fade_in\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"energy\", df[\"energy\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"key_confidence\", df[\"key_confidence\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"mode_confidence\", df[\"mode_confidence\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"release_7digitalid\", df[\"release_7digitalid\"].cast(\"Integer\"))\\\n",
    "                .withColumn(\"song_hotnesss\", df[\"song_hotnesss\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"start_of_fade_out\", df[\"start_of_fade_out\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"loudness\", df[\"loudness\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"tempo\", df[\"tempo\"].cast(\"Float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with the data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changedTypedf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with some very basic metrics over the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changedTypedf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count the number of different artists in the dataset and count the numbers of songs release every year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changedTypedf.groupBy('artist_name')\\\n",
    "             .count()\\\n",
    "             .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changedTypedf.groupBy(\"year\")\\\n",
    "             .count()\\\n",
    "             .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a bit more advanced: Calculate the average loudness of the songs of an artists tat puplished a song after 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "left = changedTypedf.select(\"artist_name\").distinct().filter(changedTypedf['year'] > 2000)\n",
    "\n",
    "right = changedTypedf.groupBy(\"artist_name\")\\\n",
    "            .agg(avg(col(\"loudness\"))\\\n",
    "            .alias(\"avg_loudness\"))\\\n",
    "            .orderBy(\"avg_loudness\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loudness = left.join(right, left.artist_name == right.artist_name)\\\n",
    "            .select(right[\"artist_name\"], \"avg_loudness\")\\\n",
    "            .orderBy(\"avg_loudness\", ascending=False)\\\n",
    "            .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
