{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from hdfs import InsecureClient\n",
    "import numpy as np\n",
    "import tables\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through the base path and find all the .h5 files that are present there then save them to a csv file in order to load them later into spark\n",
    "\n",
    "TODO: this actually needs to be reworked to create a table based on the hdfs file directory instead of local, however it doesn't matter as long as we don't change the files just run the code and pretend it does the right thing for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basedir = '../MillionSongSubset/data'\n",
    "# os.listdir(basedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of partitions we want for the rdds\n",
    "num_nodes = 3\n",
    "num_rep = 2 * num_nodes\n",
    "#sc.stop()\n",
    "conf = (SparkConf()\n",
    "   .setMaster(\"spark://192.168.2.110:7077\")\n",
    "   .setAppName(\"Group14_tab\")\n",
    "   .set(\"spark.executor.cores\", 2) # if anything >2, does not run\n",
    "   .set(\"spark.pytspark.python\",\"python3\")\n",
    "   .set(\"spark.dynamicAllocation.enabled\", False)\n",
    "   .set(\"spark.shuffle.service.enabled\", False)\n",
    "   .set(\"spark.executor.memory\", \"2g\")\n",
    "   .set(\"spark.local.dir\", \"/home/ubuntu/MillionSong/spark/tmp\"))\n",
    "\n",
    "#sc = SparkContext(conf = conf)\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to convert filenames to actual file content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good methods to explore the h5 inread: .keys(), .attrs.items() see documentation of h5py\n",
    "\n",
    "def get_title(file, idx=0):\n",
    "    return file['metadata']['songs']['title'][idx].decode(\"utf-8\")\n",
    "\n",
    "def get_artist_name(file, idx=0):\n",
    "    return file['metadata']['songs']['artist_name'][idx].decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_artist_familiarity(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist familiarity from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['artist_familiarity'][songidx]).encode('utf-8', 'ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_artist_hotttnesss(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist hotttnesss from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['artist_hotttnesss'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_artist_id(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return h5['metadata']['songs']['artist_id'][songidx].decode(\"utf-8\")\n",
    "\n",
    "def get_artist_mbid(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist musibrainz id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return h5['metadata']['songs']['artist_mbid'][songidx].decode(\"utf-8\")\n",
    "\n",
    "def get_artist_playmeid(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist playme id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['artist_playmeid'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_artist_7digitalid(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist 7digital id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['artist_7digitalid'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_artist_latitude(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist latitude from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['artist_latitude'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_artist_longitude(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist longitude from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['artist_longitude'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_artist_location(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get artist location from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return h5['metadata']['songs']['artist_location'][songidx].decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_release(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get release from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return h5['metadata']['songs']['release'][songidx].decode(\"utf-8\")\n",
    "\n",
    "def get_release_7digitalid(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get release 7digital id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['release_7digitalid'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_song_id(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get song id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return h5['metadata']['songs']['song_id'][songidx].decode(\"utf-8\")\n",
    "\n",
    "def get_song_hotttnesss(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get song hotttnesss from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['song_hotttnesss'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_track_7digitalid(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get track 7digital id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['metadata']['songs']['track_7digitalid'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_analysis_sample_rate(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get analysis sample rate from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['analysis_sample_rate'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_audio_md5(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get audio MD5 from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return h5['analysis']['songs']['audio_md5'][songidx].decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_danceability(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get danceability from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['danceability'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_duration(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get duration from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['duration'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_end_of_fade_in(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get end of fade in from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['end_of_fade_in'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_energy(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get energy from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['energy'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_key(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get key from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['key'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_key_confidence(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get key confidence from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['key_confidence'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_loudness(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get loudness from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['loudness'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_mode(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get mode from a HDF5 song file, by default the first song ifn it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['mode'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_mode_confidence(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get mode confidence from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['mode_confidence'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_start_of_fade_out(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get start of fade out from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['start_of_fade_out'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_tempo(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get tempo from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['tempo'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_time_signature(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get signature from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['time_signature'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_time_signature_confidence(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get signature confidence from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['time_signature_confidence'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_track_id(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get track id from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['analysis']['songs']['track_id'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def get_year(h5,songidx=0):\n",
    "    \"\"\"\n",
    "    Get release year from a HDF5 song file, by default the first song in it\n",
    "    \"\"\"\n",
    "    return str(h5['musicbrainz']['songs']['year'][songidx]).encode('utf-8','ignore').decode(\"utf-8\")\n",
    "\n",
    "def read(x):\n",
    "    import io\n",
    "    import h5py\n",
    "    with h5py.File(io.BytesIO(x), 'r') as f:\n",
    "        song = []\n",
    "        \n",
    "        song.append(get_artist_name(f))\n",
    "        song.append(get_title(f))\n",
    "        song.append(get_artist_familiarity(f))\n",
    "        song.append(get_artist_hotttnesss(f))\n",
    "        song.append(get_artist_id(f))\n",
    "        song.append(get_artist_mbid(f))\n",
    "        song.append(get_artist_playmeid(f))\n",
    "        song.append(get_artist_7digitalid(f))\n",
    "        song.append(get_artist_latitude(f))\n",
    "        song.append(get_artist_longitude(f))\n",
    "        song.append(get_artist_location(f))\n",
    "        song.append(get_release(f))\n",
    "        song.append(get_release_7digitalid(f))\n",
    "        song.append(get_song_id(f))\n",
    "        song.append(get_song_hotttnesss(f))\n",
    "        song.append(get_track_7digitalid(f))\n",
    "        song.append(get_analysis_sample_rate(f))\n",
    "        song.append(get_audio_md5(f))\n",
    "        song.append(get_danceability(f))\n",
    "        song.append(get_duration(f))\n",
    "        song.append(get_end_of_fade_in(f))\n",
    "        song.append(get_energy(f))\n",
    "        song.append(get_key(f))\n",
    "        song.append(get_key_confidence(f))\n",
    "        song.append(get_loudness(f))\n",
    "        song.append(get_mode(f))\n",
    "        song.append(get_mode_confidence(f))\n",
    "        song.append(get_start_of_fade_out(f))\n",
    "        song.append(get_tempo(f))\n",
    "        song.append(get_time_signature(f))\n",
    "        song.append(get_time_signature_confidence(f))\n",
    "        song.append(get_track_id(f))\n",
    "        song.append(get_year(f))\n",
    "        \n",
    "    return song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = sc.binaryFiles(\"hdfs://192.168.2.110:9000/LDSA/subset\", minPartitions=12)\n",
    "\n",
    "with open(\"/home/ubuntu/files.csv\", \"r\", newline=\"\\n\") as f:\n",
    "    results = [line.split(',')[-1][1:-2] for line in f]\n",
    "results = results[0:1000]\n",
    "\n",
    "tic = time.perf_counter()\n",
    "df = sc.union([sc.binaryFiles(\"hdfs://192.168.2.110:9000\" + f) for f in results])\n",
    "\n",
    "toc = time.perf_counter()\n",
    "print(f\"It took {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "f = sc.binaryFiles(\"hdfs://192.168.2.110:9000/LDSA/subset/A/*/*\")\n",
    "f.count()\n",
    "toc = time.perf_counter()\n",
    "print(f\"It took {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"hdfs://192.168.2.110:9000/LDSA/dataset/\"\n",
    "all_folders = [os.path.join(basedir, o) for o in os.listdir(basedir) if os.path.isdir(os.path.join(basedir,o))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "folders_ = [f\"/LDSA/dataset/{a}/{b}/{c}/\" \n",
    "            for a in string.ascii_uppercase \n",
    "            for b in string.ascii_uppercase \n",
    "            for c in string.ascii_uppercase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "folders = folders_[:10]\n",
    "df_test = sc.union([sc.binaryFiles(\"hdfs://192.168.2.110:9000\" + f) for f in folders])\n",
    "toc = time.perf_counter()\n",
    "print(f\"It took {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1000000 / 18278)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "print(getsizeof(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: fix the node so this so it works\n",
    "# df_test = df_test.repartition(12)\n",
    "df_test.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "df_mapped = df_test.map(lambda x: read(x[1]))\n",
    "df_mapped.take(1)\n",
    "toc = time.perf_counter()\n",
    "print(f\"It took {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.getNumPartitions())\n",
    "print(df_test.count())\n",
    "print(folders)\n",
    "#20-1204\n",
    "#10 - 601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: set the number of partitions here corectly as calculated above in order to maintain speed\n",
    "\n",
    "fp = \"hdfs://192.168.2.110:9000/LDSA/subset/files.csv\"\n",
    "#print(fp)\n",
    "\n",
    "\n",
    "def binary(x):\n",
    "    return sc.binaryFiles(\"hdfs://192.168.2.110:9000/LDSA/subset\" + x[25:])\n",
    "\n",
    "def h5file(x):\n",
    "    return h5py.File(io.BytesIO(x[0][1]), 'r')\n",
    "\n",
    "def trial(x):\n",
    "    with h5py.File(\"hdfs://192.168.2.110:9000/LDSA/subset\" +x[25:]) as f:\n",
    "        result = f[x[32:]]\n",
    "        return list(result[:])\n",
    "    \n",
    "\n",
    "file_paths = sc.textFile(fp, minPartitions=12)\n",
    "print(file_paths.take(1)[0][25:])\n",
    "#flatMap(lambda x: binary(x))\n",
    "#rdd = sc.union([sc.binaryFiles(f) for f in file_paths])\n",
    "\n",
    "#rdd = file_paths.flatMap(binary)\n",
    "#rdd.take(1)\n",
    "#files.take(1)\n",
    "df = sc.binaryFiles(\"hdfs://192.168.2.110:9000/LDSA/subset\", minPartitions=12) # returns a JavaPairRDD\n",
    "#ff = sqlContext.read.binaryFile(\"hdfs://192.168.2.110:9000/LDSA/subset\")   \n",
    "#print(df.take(1))\n",
    "#df2=df.flatMap(h5file)\n",
    "#df2 = df.map(lambda x: h5file(x))\n",
    "#print(df2.take(1))\n",
    "print(df)\n",
    "\n",
    "\n",
    "files = df.map(lambda x: foo(x[1]))  \n",
    "\n",
    "def foo(data):  \n",
    "    with io.BytesIO(data) as b, h5py.File(b, 'r') as f:  \n",
    "        return list(f) # [\"analysis\", \"metadata\", \"musicbrainz\"]  \n",
    "\n",
    "#print(files.collect())\n",
    "\n",
    "\n",
    "## Old version\n",
    "#file_paths = sc.textFile(fp, minPartitions=12).map(lambda x: f2(x))\n",
    "#print(file_paths)\n",
    "\n",
    "\n",
    "## One file test\n",
    "#file_paths = sc.binaryFiles(\"hdfs://192.168.2.110:9000/LDSA/subset/A/E/E/TRAEELO128F425BD8F.h5\")\n",
    "#a = h5py.File(io.BytesIO(file_paths.take(1)[0][1]), 'r')\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Convert rdd to dataframe\n",
    "convert the rdd (api version 1) to a dataframe (api version 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "columns = ['artist_name', 'title', 'artist_familiarity', \n",
    "             'artist_hotttnesss', 'artist_id', 'artist_mbid', \n",
    "             'artist_playmeid', 'artist_7digitalid', 'artist_latitude', \n",
    "             'artist_longitude', 'artist_location', 'release', \n",
    "             'release_7digitalid', \n",
    "             'song_id', 'song_hotnesss', 'track_7digitalid', \n",
    "             'analysis_sample_rate', \n",
    "             'audio_md5', 'danceability', 'duration', 'end_of_fade_in', \n",
    "             'energy', 'key', \n",
    "             'key_confidence', 'loudness', 'mode', 'mode_confidence', \n",
    "             'start_of_fade_out', \n",
    "             'tempo', 'time_signature', \n",
    "             'time_signature_confidence', 'track_id', 'year']\n",
    "df = df_mapped.toDF(columns)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "df.printSchema()\n",
    "print(f\"It took {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "# change types\n",
    "from pyspark.sql import types \n",
    "['BinaryType', 'BooleanType', 'ByteType', 'DateType', \n",
    "          'DecimalType', 'DoubleType', 'FloatType', 'IntegerType', \n",
    "           'LongType', 'ShortType', 'StringType', 'TimestampType']\n",
    "changedTypedf = df.withColumn(\"year\", df[\"year\"].cast(\"Integer\"))\\\n",
    "                .withColumn(\"track_id\", df[\"track_id\"].cast(\"Integer\"))\\\n",
    "                .withColumn(\"artist_id\", df[\"artist_id\"].cast(\"Integer\"))\\\n",
    "                .withColumn(\"song_id\", df[\"song_id\"].cast(\"Integer\"))\\\n",
    "                .withColumn(\"duration\", df[\"duration\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"danceability\", df[\"danceability\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"end_of_fade_in\", df[\"end_of_fade_in\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"energy\", df[\"energy\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"key_confidence\", df[\"key_confidence\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"mode_confidence\", df[\"mode_confidence\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"release_7digitalid\", df[\"release_7digitalid\"].cast(\"Integer\"))\\\n",
    "                .withColumn(\"song_hotnesss\", df[\"song_hotnesss\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"start_of_fade_out\", df[\"start_of_fade_out\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"loudness\", df[\"loudness\"].cast(\"Float\"))\\\n",
    "                .withColumn(\"tempo\", df[\"tempo\"].cast(\"Float\"))\n",
    "\n",
    "toc = time.perf_counter()\n",
    "print(f\"It took {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with the data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changedTypedf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with some very basic metrics over the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changedTypedf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count the number of different artists in the dataset and count the numbers of songs release every year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "changedTypedf.groupBy('artist_name')\\\n",
    "             .count()\\\n",
    "             .show()\n",
    "        \n",
    "toc = time.perf_counter()\n",
    "print(f\"It took {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "changedTypedf.groupBy(\"year\")\\\n",
    "             .count()\\\n",
    "             .show()\n",
    "        \n",
    "toc = time.perf_counter()\n",
    "print(f\"It took {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a bit more advanced: Calculate the average loudness of the songs of an artists tat puplished a song after 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "from pyspark.sql.functions import col, avg\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "left = changedTypedf.select(\"artist_name\").distinct().filter(changedTypedf['year'] > 2000)\n",
    "\n",
    "right = changedTypedf.groupBy(\"artist_name\")\\\n",
    "            .agg(avg(col(\"loudness\"))\\\n",
    "            .alias(\"avg_loudness\"))\\\n",
    "            .orderBy(\"avg_loudness\", ascending=False)\n",
    "\n",
    "avg_loudness = left.join(right, left.artist_name == right.artist_name)\\\n",
    "            .select(right[\"artist_name\"], \"avg_loudness\")\\\n",
    "            .orderBy(\"avg_loudness\", ascending=False)\\\n",
    "            .collect()\n",
    "        \n",
    "toc = time.perf_counter()\n",
    "print(f\"It took {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
